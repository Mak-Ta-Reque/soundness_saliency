{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import copy\n",
    "from sklearn import preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/mnt/sda/abka03-data/tabular/creditcard.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the dat\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "\n",
    "# Load the tabular data into a pandas DataFrame\n",
    "data = pd.read_csv(data_path)\n",
    "data = data.drop(\"Time\", axis=1)\n",
    "# Extract the numerical columns that need normalization\n",
    "num_cols = data.select_dtypes(include=[float]).columns.tolist()\n",
    "\n",
    "\n",
    "# Initialize a MinMaxScaler object to perform normalization\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "# Fit the scaler on the numerical data\n",
    "scaler.fit(data[num_cols])\n",
    "\n",
    "# Transform the numerical data using the scaler\n",
    "data[num_cols] = scaler.transform(data[num_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0  0.976322  0.767319  0.850990  0.311263  0.762616  0.264534  0.263765   \n",
      "1  0.958601  0.733538  0.804513  0.343065  0.759797  0.264888  0.272922   \n",
      "2  0.974030  0.767509  0.836418  0.314862  0.764396  0.253305  0.269284   \n",
      "3  0.956106  0.727384  0.816184  0.222135  0.753654  0.260515  0.268382   \n",
      "4  0.995141  0.749913  0.824406  0.179431  0.756354  0.261842  0.256615   \n",
      "\n",
      "         V8        V9       V10  ...       V21       V22       V23       V24  \\\n",
      "0  0.788012  0.472322  0.509301  ...  0.558600  0.490594  0.666679  0.408058   \n",
      "1  0.781364  0.477063  0.502258  ...  0.572271  0.501434  0.654863  0.480484   \n",
      "2  0.782594  0.451105  0.508815  ...  0.563793  0.517828  0.662995  0.464377   \n",
      "3  0.779979  0.397560  0.528787  ...  0.565941  0.477886  0.650945  0.329075   \n",
      "4  0.786358  0.422571  0.544315  ...  0.560174  0.515199  0.669525  0.471044   \n",
      "\n",
      "        V25       V26       V27       V28    Amount  Class  \n",
      "0  0.596266  0.339114  0.417313  0.313479  0.000836      0  \n",
      "1  0.559589  0.295414  0.413714  0.316124  0.039891      0  \n",
      "2  0.613713  0.376993  0.416129  0.313855  0.005410      0  \n",
      "3  0.604302  0.407934  0.413716  0.316290  0.035166      0  \n",
      "4  0.557806  0.388410  0.416669  0.312230  0.001907      0  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "Total number of data: 284807\n",
      "Total number of features: 29\n",
      "Train data size: 170885\n",
      "Test data size: 56962\n",
      "Validation data size: 56962\n",
      "Fetures name in the data: V1,V2,V3,V4,V5,V6,V7,V8,V9,V10,V11,V12,V13,V14,V15,V16,V17,V18,V19,V20,V21,V22,V23,V24,V25,V26,V27,V28,Amount,Class\n"
     ]
    }
   ],
   "source": [
    "#credit_card_data = pd.read_csv(data_path)\n",
    "# Load the tabular data into a pandas DataFrame\n",
    "\n",
    "# Extract the numerical columns that need normalization\n",
    "\n",
    "# Shuffle the data\n",
    "credit_card_data = data.sample(frac=1).reset_index(drop=True)\n",
    "print(credit_card_data.head())\n",
    "print(f\"Total number of data: {len(credit_card_data)}\")\n",
    "print(f\"Total number of features: {credit_card_data.shape[1] -1}\")\n",
    "# Split the dataset into training test and validation set\n",
    "train_size = int(0.6 * len(credit_card_data))\n",
    "val_size = int(0.2 * len(credit_card_data))\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "train_data = credit_card_data.loc[:train_size]\n",
    "val_data = credit_card_data.loc[train_size:train_size+val_size]\n",
    "test_data = credit_card_data.loc[train_size+val_size:]\n",
    "\n",
    "print(f\"Train data size: {len(train_data)}\")\n",
    "print(f\"Test data size: {len(test_data)}\")\n",
    "print(f\"Validation data size: {len(val_data)}\")\n",
    "\n",
    "# Drop the time from the train data\n",
    "#train_data = train_data.drop(\"Time\", axis=1)\n",
    "#test_data = test_data.drop(\"Time\", axis=1)\n",
    "#val_data = val_data.drop(\"Time\", axis=1)\n",
    "print(f\"Fetures name in the data: {','.join(list(train_data.columns))}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0  0.976322  0.767319  0.850990  0.311263  0.762616  0.264534  0.263765   \n",
      "1  0.958601  0.733538  0.804513  0.343065  0.759797  0.264888  0.272922   \n",
      "2  0.974030  0.767509  0.836418  0.314862  0.764396  0.253305  0.269284   \n",
      "3  0.956106  0.727384  0.816184  0.222135  0.753654  0.260515  0.268382   \n",
      "4  0.995141  0.749913  0.824406  0.179431  0.756354  0.261842  0.256615   \n",
      "\n",
      "         V8        V9       V10  ...       V21       V22       V23       V24  \\\n",
      "0  0.788012  0.472322  0.509301  ...  0.558600  0.490594  0.666679  0.408058   \n",
      "1  0.781364  0.477063  0.502258  ...  0.572271  0.501434  0.654863  0.480484   \n",
      "2  0.782594  0.451105  0.508815  ...  0.563793  0.517828  0.662995  0.464377   \n",
      "3  0.779979  0.397560  0.528787  ...  0.565941  0.477886  0.650945  0.329075   \n",
      "4  0.786358  0.422571  0.544315  ...  0.560174  0.515199  0.669525  0.471044   \n",
      "\n",
      "        V25       V26       V27       V28    Amount  Class  \n",
      "0  0.596266  0.339114  0.417313  0.313479  0.000836      0  \n",
      "1  0.559589  0.295414  0.413714  0.316124  0.039891      0  \n",
      "2  0.613713  0.376993  0.416129  0.313855  0.005410      0  \n",
      "3  0.604302  0.407934  0.413716  0.316290  0.035166      0  \n",
      "4  0.557806  0.388410  0.416669  0.312230  0.001907      0  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "class_identifier = train_data.loc[train_data['Class'] == 0]\n",
    "print(class_identifier.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pytorch data generator for tabular dat\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import random\n",
    "class PandasDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, supervised=False, train=False):\n",
    "        self.data = dataframe.values\n",
    "        self.supervised = supervised\n",
    "        self.train = train\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def random_dropout(self, data, dropout_prob):\n",
    "        mask = torch.tensor(np.random.binomial(n=1, p=1-dropout_prob, size=data.shape))\n",
    "        masked_data = data * mask.float()\n",
    "        return masked_data\n",
    "    def pow_augment(self, data,  p):\n",
    "        masked_data = torch.pow(data, p)\n",
    "        return masked_data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #x = (torch.from_numpy(self.data[index, :-1]).float() - np.min(self.data[index, :-1]) )/ (np.max(self.data[index, :-1]) - np.min(self.data[index, :-1]))\n",
    "        x = torch.from_numpy(self.data[index, :-1]).float()\n",
    "        if self.train:\n",
    "            x = self.random_dropout(x, 0.5)\n",
    "            random_number = round(random.uniform(0.9, 1), 2)\n",
    "            x = self.pow_augment(x, random_number)\n",
    "        if self.supervised:\n",
    "            y = torch.tensor(self.data[index, -1]).float()\n",
    "        else:\n",
    "            y = -1\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170885\n",
      "tensor([[9.9118e-01, 7.8857e-01, 8.4149e-01, 4.5792e-01, 0.0000e+00, 2.9687e-01,\n",
      "         2.9448e-01, 8.0071e-01, 5.0837e-01, 5.6090e-01, 0.0000e+00, 0.0000e+00,\n",
      "         5.5702e-01, 7.2309e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.0204e-01,\n",
      "         5.0379e-01, 0.0000e+00, 5.8726e-01, 5.4890e-01, 0.0000e+00, 0.0000e+00,\n",
      "         6.0958e-01, 4.6525e-01, 4.4564e-01, 3.4215e-01, 5.6532e-04]]) tensor([0.])\n",
      "tensor([[0.9722, 0.0000, 0.8637, 0.0000, 0.7551, 0.0000, 0.0000, 0.7920, 0.0000,\n",
      "         0.5478, 0.0000, 0.0000, 0.5005, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4223, 0.0000,\n",
      "         0.0000, 0.0000]]) tensor([0.])\n",
      "tensor([[0.9829, 0.0000, 0.0000, 0.2067, 0.0000, 0.2621, 0.2709, 0.7881, 0.0000,\n",
      "         0.5473, 0.0000, 0.7009, 0.0000, 0.6400, 0.3889, 0.0000, 0.7536, 0.6323,\n",
      "         0.5504, 0.0000, 0.0000, 0.0000, 0.0000, 0.4845, 0.5924, 0.3538, 0.4282,\n",
      "         0.3250, 0.0000]]) tensor([0.])\n",
      "tensor([[0.9472, 0.0000, 0.8834, 0.3772, 0.7820, 0.2956, 0.2952, 0.8008, 0.4923,\n",
      "         0.5353, 0.3657, 0.6276, 0.0000, 0.0000, 0.1238, 0.0000, 0.0000, 0.7330,\n",
      "         0.3562, 0.5984, 0.0000, 0.5706, 0.0000, 0.0000, 0.5856, 0.0000, 0.4427,\n",
      "         0.0000, 0.0041]]) tensor([0.])\n",
      "tensor([[0.0000, 0.7725, 0.0000, 0.2890, 0.7729, 0.2578, 0.2701, 0.0000, 0.4911,\n",
      "         0.4901, 0.2542, 0.6984, 0.4426, 0.5591, 0.0000, 0.4673, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.5641, 0.0000, 0.0000, 0.4311, 0.5959, 0.5446, 0.0000,\n",
      "         0.0000, 0.0000]]) tensor([0.])\n",
      "tensor([[9.4471e-01, 7.9262e-01, 0.0000e+00, 0.0000e+00, 7.7951e-01, 0.0000e+00,\n",
      "         0.0000e+00, 7.9983e-01, 4.8525e-01, 0.0000e+00, 0.0000e+00, 7.1554e-01,\n",
      "         0.0000e+00, 6.5874e-01, 4.0801e-01, 4.8374e-01, 0.0000e+00, 0.0000e+00,\n",
      "         5.7678e-01, 6.0211e-01, 5.7706e-01, 0.0000e+00, 0.0000e+00, 3.5052e-01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 3.4006e-01, 3.9598e-04]]) tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "# Assuming you have a Pandas DataFrame called 'data'\n",
    "dataset = PandasDataset(train_data, supervised=True, train=True)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "# print 5 samples of data\n",
    "print(len(dataloader.dataset))\n",
    "for batch_idx, (data, target) in enumerate(dataloader):\n",
    "    print(data, target)\n",
    "    if batch_idx > 4:\n",
    "        break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class AutoEncoder(nn.Module) :\n",
    "    def __init__(self, rep_dim=10, input_shape=30, dim = 128):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        ## Encoder\n",
    "        self.rep_dim = rep_dim\n",
    "        self.fc1 = nn.Linear(input_shape, dim,  bias=False)\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        self.bn1 = nn.BatchNorm1d(dim, eps=1e-04, affine=False)\n",
    "        self.fc2 = nn.Linear(dim, self.rep_dim, bias=False)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        self.m = nn.Softmax(dim=1)\n",
    "\n",
    "        ## Decoder\n",
    "        self.fc3 = nn.Linear(self.rep_dim, dim,  bias=False)\n",
    "        nn.init.xavier_uniform_(self.fc3.weight)\n",
    "        self.bn2 = nn.BatchNorm1d(dim, eps=1e-04, affine=False)\n",
    "        self.fc4 = nn.Linear(dim,  input_shape,  bias=False)\n",
    "        nn.init.xavier_uniform_(self.fc4.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encode = self.fc2(self.relu(self.fc1(x)))\n",
    "        encode = self.m(encode)\n",
    "        # create a sample Torch array\n",
    "        arr = torch.sum(encode, dim=1)\n",
    "\n",
    "\n",
    "        # check if any value in the array is less than 1\n",
    "        if torch.min(arr) < .99 :\n",
    "\n",
    "            # if yes, raise an exception\n",
    "            raise ValueError(\"Array contains a value less than 1\")\n",
    "        if torch.max(arr) >  1.10 :\n",
    "\n",
    "            # if yes, raise an exception\n",
    "            raise ValueError(\"Array contains a value greater than 1\")\n",
    "\n",
    "\n",
    "        decode = self.fc4(self.relu(self.fc3(encode)))\n",
    "        return encode, decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "log_interval = 1000\n",
    "lam = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/170885 (0%)]\tLoss: 0.177270\n",
      "\n",
      "Validation set: Average loss: 0.0003, Accuracy: 56866/56962 (99.831467%)\n",
      "Train Epoch: 1 [0/170885 (0%)]\tLoss: 0.176417\n",
      "\n",
      "Validation set: Average loss: 0.0003, Accuracy: 56866/56962 (99.831467%)\n",
      "Train Epoch: 2 [0/170885 (0%)]\tLoss: 0.175605\n",
      "\n",
      "Validation set: Average loss: 0.0003, Accuracy: 56866/56962 (99.831467%)\n",
      "Train Epoch: 3 [0/170885 (0%)]\tLoss: 0.174491\n",
      "\n",
      "Validation set: Average loss: 0.0003, Accuracy: 56866/56962 (99.831467%)\n",
      "Train Epoch: 4 [0/170885 (0%)]\tLoss: 0.173554\n",
      "\n",
      "Validation set: Average loss: 0.0003, Accuracy: 56866/56962 (99.831467%)\n",
      "Train Epoch: 5 [0/170885 (0%)]\tLoss: 0.173514\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-285-b281e7e5d61b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlam\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ml1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# Update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/thesis-env/lib/python3.6/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/thesis-env/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming you have a PyTorch dataset and model defined\n",
    "\n",
    "# Set device (GPU or CPU)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate model\n",
    "model = AutoEncoder(rep_dim=2, input_shape=29)\n",
    "model.to(device)\n",
    "\n",
    "# Define loss criterion and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "# Instantiate data loader\n",
    "dataset = PandasDataset(train_data, supervised=True, train=True)\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=10000, shuffle=True)\n",
    "val_dataset = PandasDataset(val_data, supervised=True)\n",
    "val_loader = DataLoader(dataset=val_dataset , batch_size=1000, shuffle=False)\n",
    "\n",
    "# Training loop\n",
    "best_acc = 0.0\n",
    "for epoch in range(num_epochs):\n",
    "    # Set model to train mode\n",
    "    model.train()\n",
    "\n",
    "    # Iterate over batches\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        # Load data to device\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = model(data)\n",
    "        outputs = predictions[1]\n",
    "        pred_probs = predictions[0]\n",
    "\n",
    "\n",
    "        # Calculate loss\n",
    "        l1 = criterion(outputs, data) \n",
    "        l2 = 0.0\n",
    "        #print(pred_probs)\n",
    "        #print(torch.max(pred_probs))\n",
    "        for it in [(-pred_prob * torch.log2(pred_prob)).sum() for pred_prob in pred_probs]:\n",
    "            l2 += it\n",
    "        loss = lam * l1 + (1.0-lam) * l2\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step() # Print progress\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "    # Evaluate on validation set after each epoch (optional)\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        class_sample = class_identifier.sample(n=5000)\n",
    "        identifiers_sample = PandasDataset(class_sample, supervised=True)\n",
    "        identifiers_loader = DataLoader(dataset=identifiers_sample, batch_size=5000, shuffle=True)\n",
    "        id_samples = next(iter(identifiers_loader))\n",
    "        org_nonanomaly_class = id_samples[1]\n",
    "        org_bins = torch.bincount(org_nonanomaly_class.int())\n",
    "        org_nonanomaly_class  = torch.argmax(org_bins).item()\n",
    "        nonanomaly_class = model(id_samples[0].to(device))[0]\n",
    "        nonanomaly_class = nonanomaly_class.argmax(dim=1, keepdim=True).view(-1)\n",
    "        bins = torch.bincount(nonanomaly_class)\n",
    "        nonanomaly_class  = torch.argmax(bins).item()\n",
    "        code = {nonanomaly_class: org_nonanomaly_class, org_nonanomaly_class: nonanomaly_class}\n",
    "\n",
    "\n",
    "        for data, target in val_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(data)\n",
    "            out_pred = output[0]\n",
    "            output = output[1]\n",
    "            val_loss += criterion(output, data).item()\n",
    "            pred = out_pred.argmax(dim=1, keepdim=True)\n",
    "            if nonanomaly_class == org_nonanomaly_class: \n",
    "                output_tensor = pred\n",
    "                \n",
    "            else:\n",
    "                output_tensor = torch.tensor([[code[x[0].item()]] for x in pred]).to(device)\n",
    "\n",
    "            #print(out_pred.shape)\n",
    "            #print(f\"Prediction: {pred[0]} Target: {target[0]}\")\n",
    "            correct += output_tensor.eq(target.view_as(output_tensor)).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        accuracy = 100. * correct / len(val_loader.dataset)\n",
    "        if accuracy > best_acc:\n",
    "            best_acc = accuracy\n",
    "            # save the state dictionary\n",
    "            torch.save(model.state_dict(), 'model_weights.pth')\n",
    "\n",
    "\n",
    "\n",
    "        print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.6f}%)'.format(\n",
    "            val_loss, correct, len(val_loader.dataset), accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = PandasDataset(test_data, supervised=True)\n",
    "test_loader = DataLoader(dataset=test_dataset , batch_size=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.79811102138268\n",
      "[0.44797641038894653, 0.5520235896110535]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# define your model\n",
    "model = AutoEncoder(rep_dim=2, input_shape=29).to(device)\n",
    "\n",
    "# load the state dictionary\n",
    "model.load_state_dict(torch.load('model_weights.pth'))\n",
    "model.eval()\n",
    "val_loss = 0\n",
    "correct = 0\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    class_sample = class_identifier.sample(n=5000)\n",
    "    identifiers_sample = PandasDataset(class_sample, supervised=True)\n",
    "    identifiers_loader = DataLoader(dataset=identifiers_sample, batch_size=5000, shuffle=True)\n",
    "    id_samples = next(iter(identifiers_loader))\n",
    "    org_nonanomaly_class = id_samples[1]\n",
    "    org_bins = torch.bincount(org_nonanomaly_class.int())\n",
    "    org_nonanomaly_class  = torch.argmax(org_bins).item()\n",
    "    nonanomaly_class = model(id_samples[0].to(device))[0]\n",
    "    nonanomaly_class = nonanomaly_class.argmax(dim=1, keepdim=True).view(-1)\n",
    "    bins = torch.bincount(nonanomaly_class)\n",
    "    nonanomaly_class  = torch.argmax(bins).item()\n",
    "    code = {nonanomaly_class: org_nonanomaly_class, org_nonanomaly_class: nonanomaly_class}\n",
    "    for data, target in test_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(data)\n",
    "        out_pred = output[0]\n",
    "        output = output[1]\n",
    "        predictions = predictions + out_pred.detach().cpu().tolist()\n",
    "        val_loss += criterion(output, data).item()\n",
    "        pred = out_pred.argmax(dim=1, keepdim=True)\n",
    "        if nonanomaly_class == org_nonanomaly_class: \n",
    "            output_tensor = pred\n",
    "            \n",
    "        else:\n",
    "            output_tensor = torch.tensor([[code[x[0].item()]] for x in pred]).to(device)\n",
    "\n",
    "        #print(out_pred.shape)\n",
    "        #print(f\"Prediction: {pred[0]} Target: {target[0]}\")\n",
    "        correct += output_tensor.eq(target.view_as(output_tensor)).sum().item()\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(accuracy)\n",
    "\n",
    "    print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.646\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create 2D list\n",
    "data = predictions\n",
    "\n",
    "\n",
    "# create x and y lists from the 2D list\n",
    "x = [round(row[0],3) for row in data]\n",
    "y = [round(row[1],3) for row in data]\n",
    "print(max(y))\n",
    "# create scatter plot\n",
    "plt.scatter(x, y)\n",
    "\n",
    "# set title and axis labels\n",
    "plt.title('Scatter Plot')\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "\n",
    "# display the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

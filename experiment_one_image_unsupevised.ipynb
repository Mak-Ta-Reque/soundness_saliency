{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from certificate_methods import *\n",
    "from utils import ReducedModel, parse\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "import cv2\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.1+cu102\n",
      "0.11.2+cu102\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def scale_batch_images(batch, min_val=0, max_val=1):\n",
    "    \"\"\"\n",
    "    Scales a batch of PyTorch tensors (images) to the specified range [min_val, max_val].\n",
    "\n",
    "    Args:\n",
    "        batch (torch.Tensor): A batch of PyTorch tensors (images) to be scaled with shape (batch_size, channels, height, width)\n",
    "        min_val (float, optional): Minimum value of the target range. Defaults to 0.\n",
    "        max_val (float, optional): Maximum value of the target range. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A new batch of PyTorch tensors with values scaled to the specified range\n",
    "    \"\"\"\n",
    "\n",
    "    batch_min = torch.amin(batch, dim=(1, 2, 3), keepdim=True)\n",
    "    batch_max = torch.amax(batch, dim=(1, 2, 3), keepdim=True)\n",
    "\n",
    "    # Normalize the batch of images to [0, 1] range\n",
    "    normalized_batch = (batch - batch_min) / (batch_max - batch_min)\n",
    "\n",
    "    # Scale the normalized batch to the target range [min_val, max_val]\n",
    "    scaled_batch = normalized_batch * (max_val - min_val) + min_val\n",
    "\n",
    "    return scaled_batch\n",
    "\n",
    "def man_transform(masked_imgs):\n",
    "    transformed_images = []\n",
    "    for img in masked_imgs:\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(1, -1, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(1, -1, 1, 1)\n",
    "        timg = torch.tensor(img).unsqueeze(0)  # Add an extra dimension to make it a batch of size 1\n",
    "    \n",
    "\n",
    "        normalized_image = (timg/torch.max(timg)- mean) / std\n",
    "        normalized_image = normalized_image [0]\n",
    "    \n",
    "\n",
    "        transformed_images.append(normalized_image)\n",
    "    stacked_images = torch.stack(transformed_images, dim=0)\n",
    "    return stacked_images\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'input_batch' is a PyTorch tensor of shape (batch_size, channels, height, width)\n",
    "# scaled_batch = scale_batch_images(input_batch, 0, 1)\n",
    "def evaluate(model, images, labels):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        output_probs = torch.nn.Softmax(dim = 1)(outputs).detach().cpu().numpy()\n",
    "    output_labels = np.argmax(output_probs, axis = 1)\n",
    "    correct = np.sum(output_labels == labels.numpy())\n",
    "    total = len(labels)\n",
    "    print(f\" Accuracy: {correct}/{total}: {correct / total}\")\n",
    "def prediction_probs(model, input_data):\n",
    "    prob_dicts = []\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_data.cuda())\n",
    "        output_probs = torch.nn.Softmax(dim = 1)(outputs).detach().cpu().numpy()\n",
    "        print(output_probs.shape)\n",
    "\n",
    "    output_labels = np.argmax(output_probs, axis = 1)\n",
    "    for it in output_labels:\n",
    "        prob_dicts.append({it: output_probs[0][it]})\n",
    "        \n",
    "    return prob_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.FC_input = nn.Linear(input_dim, hidden_dim)\n",
    "        self.FC_input2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.FC_mean  = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.FC_var   = nn.Linear (hidden_dim, latent_dim)\n",
    "        \n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        self.training = True\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_       = self.LeakyReLU(self.FC_input(x))\n",
    "        h_       = self.LeakyReLU(self.FC_input2(h_))\n",
    "        mean     = self.FC_mean(h_)\n",
    "        log_var  = self.FC_var(h_)                     # encoder produces mean and log of variance \n",
    "                                                       #             (i.e., parateters of simple tractable normal distribution \"q\"\n",
    "        \n",
    "        return mean, log_var\n",
    "# LOad trianed weight\n",
    "x_dim  = 784\n",
    "hidden_dim = 400\n",
    "latent_dim = 10\n",
    "batch_size = 1\n",
    "model = Encoder(input_dim=x_dim, hidden_dim=hidden_dim, latent_dim=latent_dim)\n",
    "\n",
    "model.load_state_dict(torch.load(\"Pytorch-VAE-tutorial/enocde_weight.pth\"))\n",
    "\n",
    "model = model.eval()\n",
    "\n",
    "class Unsupervised(nn.Module):\n",
    "    \n",
    "    def __init__(self, basemodel):\n",
    "        super(Unsupervised, self).__init__()\n",
    "        self.encoder = basemodel\n",
    "        self.m = nn.Softmax(dim=1)\n",
    " \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(1, x_dim)\n",
    "        mean, var = self.encoder(x)\n",
    "        z = mean + var\n",
    "        z = self.m(z)\n",
    "        return z\n",
    "unsup = Unsupervised(basemodel=model)\n",
    "unsup = unsup.eval()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset_path = '~/datasets'\n",
    "batch_size = 5\n",
    "mnist_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} \n",
    "\n",
    "train_dataset = MNIST(dataset_path, transform=mnist_transform, train=True, download=True)\n",
    "test_dataset  = MNIST(dataset_path, transform=mnist_transform, train=False, download=True)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader  = DataLoader(dataset=test_dataset,  batch_size=batch_size, shuffle=False, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 28, 28])\n",
      "torch.Size([1, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "sample, class_num  = iter(test_loader).next()\n",
    "print(sample.shape)\n",
    "sample = sample[0:1]\n",
    "class_num = class_num[0:1]\n",
    "print(sample.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load sample images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10)\n",
      "The output cluster and the probability of them:  [{3: 0.23196931}]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAKYElEQVR4nO3dX+jdd33H8edrbZpidJDoFkIt00kZlMGi/MgGluHolNqb1BsxF5JB4eeFBQUvLO7CXpYxlV0MIa7BbLjKQEtzUTazIBRhlP5asjZtnaklYkKaTHphHSxN63sXv2/lZ/v75ffrOd/zh72fDzicc77f88v3zaHPnnO+58AnVYWk//9+Z9EDSJoPY5eaMHapCWOXmjB2qYkb53mwm7K7bmbPPA8ptfK//A+v1dVstm+q2JPcBfwdcAPwD1X14PUefzN7+NPcOc0hJV3HE3V6y30Tv41PcgPw98AngduBI0lun/TfkzRb03xmPwS8WFUvVdVrwHeBw+OMJWls08R+C/DzDfcvDNt+S5LVJGtJ1q5xdYrDSZrGzM/GV9WxqlqpqpVd7J714SRtYZrYLwK3brj//mGbpCU0TexPArcl+WCSm4DPACfHGUvS2Cb+6q2qXk9yH/BvrH/1dryqnhttMkmjmup79qp6DHhspFkkzZA/l5WaMHapCWOXmjB2qQljl5owdqkJY5eaMHapCWOXmjB2qQljl5owdqkJY5eaMHapCWOXmjB2qQljl5owdqkJY5eaMHapCWOXmjB2qQljl5owdqkJY5eaMHapCWOXmjB2qQljl5owdqmJqZZsTnIeeBV4A3i9qlbGGErS+KaKffAXVfWLEf4dSTPk23ipiWljL+AHSZ5KsrrZA5KsJllLsnaNq1MeTtKkpn0bf0dVXUzy+8CpJD+uqsc3PqCqjgHHAH43+2rK40ma0FSv7FV1cbi+AjwCHBpjKEnjmzj2JHuSvOfN28AngLNjDSZpXNO8jd8PPJLkzX/nn6vqX0eZStLoJo69ql4C/mTEWSTNkF+9SU0Yu9SEsUtNGLvUhLFLTRi71ISxS00Yu9SEsUtNGLvUhLFLTRi71ISxS00Yu9SEsUtNGLvUhLFLTRi71ISxS00Yu9SEsUtNGLvUhLFLTRi71ISxS00Yu9SEsUtNGLvUhLFLTRi71MS2sSc5nuRKkrMbtu1LcirJueF672zHlDStnbyyfxu46y3b7gdOV9VtwOnhvqQltm3sVfU48MpbNh8GTgy3TwD3jDuWpLHdOOHf7a+qS8Ptl4H9Wz0wySqwCnAz75rwcJKmNfUJuqoqoK6z/1hVrVTVyi52T3s4SROaNPbLSQ4ADNdXxhtJ0ixMGvtJ4Ohw+yjw6DjjSJqVnXz19jDwH8AfJbmQ5F7gQeDjSc4Bfzncl7TEtj1BV1VHtth158izSJohf0EnNWHsUhPGLjVh7FITxi41YexSE8YuNWHsUhPGLjVh7FITxi41YexSE8YuNWHsUhPGLjVh7FITxi41YexSE8YuNWHsUhPGLjVh7FITxi41YexSE8YuNWHsUhPGLjVh7FITxi41YexSEztZn/14kitJzm7Y9kCSi0nODJe7ZzumpGnt5JX928Bdm2z/RlUdHC6PjTuWpLFtG3tVPQ68ModZJM3QNJ/Z70vyzPA2f+9WD0qymmQtydo1rk5xOEnTmDT2bwIfAg4Cl4CvbfXAqjpWVStVtbKL3RMeTtK0Joq9qi5X1RtV9WvgW8ChcceSNLaJYk9yYMPdTwFnt3qspOVw43YPSPIw8DHgfUkuAF8FPpbkIFDAeeBzsxtR0hi2jb2qjmyy+aEZzCJphvwFndSEsUtNGLvUhLFLTRi71ISxS00Yu9SEsUtNGLvUhLFLTRi71ISxS00Yu9SEsUtNGLvUhLFLTRi71ISxS00Yu9SEsUtNGLvUhLFLTRi71ISxS00Yu9SEsUtNGLvUhLFLTRi71ISxS01sG3uSW5P8MMnzSZ5L8oVh+74kp5KcG673zn5cSZPaySv768CXqup24M+Azye5HbgfOF1VtwGnh/uSltS2sVfVpap6erj9KvACcAtwGDgxPOwEcM+MZpQ0ghvfyYOTfAD4MPAEsL+qLg27Xgb2b/E3q8AqwM28a+JBJU1nxyfokrwb+B7wxar65cZ9VVVAbfZ3VXWsqlaqamUXu6caVtLkdhR7kl2sh/6dqvr+sPlykgPD/gPAldmMKGkMOzkbH+Ah4IWq+vqGXSeBo8Pto8Cj448naSw7+cz+UeCzwLNJzgzbvgI8CPxLknuBnwGfnsmEkkaxbexV9SMgW+y+c9xxJM2Kv6CTmjB2qQljl5owdqkJY5eaMHapCWOXmjB2qQljl5owdqkJY5eaMHapCWOXmjB2qQljl5owdqkJY5eaMHapCWOXmjB2qQljl5owdqkJY5eaMHapCWOXmjB2qQljl5owdqkJY5ea2Mn67Lcm+WGS55M8l+QLw/YHklxMcma43D37cSVNaifrs78OfKmqnk7yHuCpJKeGfd+oqr+d3XiSxrKT9dkvAZeG268meQG4ZdaDSRrXO/rMnuQDwIeBJ4ZN9yV5JsnxJHu3+JvVJGtJ1q5xdbppJU1sx7EneTfwPeCLVfVL4JvAh4CDrL/yf22zv6uqY1W1UlUru9g9/cSSJrKj2JPsYj3071TV9wGq6nJVvVFVvwa+BRya3ZiSprWTs/EBHgJeqKqvb9h+YMPDPgWcHX88SWPZydn4jwKfBZ5NcmbY9hXgSJKDQAHngc/NYD5JI9nJ2fgfAdlk12PjjyNpVvwFndSEsUtNGLvUhLFLTRi71ISxS00Yu9SEsUtNGLvUhLFLTRi71ISxS00Yu9SEsUtNpKrmd7Dkv4Gfbdj0PuAXcxvgnVnW2ZZ1LnC2SY052x9U1e9ttmOusb/t4MlaVa0sbIDrWNbZlnUucLZJzWs238ZLTRi71MSiYz+24ONfz7LOtqxzgbNNai6zLfQzu6T5WfQru6Q5MXapiYXEnuSuJP+V5MUk9y9ihq0kOZ/k2WEZ6rUFz3I8yZUkZzds25fkVJJzw/Wma+wtaLalWMb7OsuML/S5W/Ty53P/zJ7kBuAnwMeBC8CTwJGqen6ug2whyXlgpaoW/gOMJH8O/Ar4x6r642Hb3wCvVNWDw/8o91bVl5dktgeAXy16Ge9htaIDG5cZB+4B/ooFPnfXmevTzOF5W8Qr+yHgxap6qapeA74LHF7AHEuvqh4HXnnL5sPAieH2Cdb/Y5m7LWZbClV1qaqeHm6/Cry5zPhCn7vrzDUXi4j9FuDnG+5fYLnWey/gB0meSrK66GE2sb+qLg23Xwb2L3KYTWy7jPc8vWWZ8aV57iZZ/nxanqB7uzuq6iPAJ4HPD29Xl1KtfwZbpu9Od7SM97xsssz4byzyuZt0+fNpLSL2i8CtG+6/f9i2FKrq4nB9BXiE5VuK+vKbK+gO11cWPM9vLNMy3pstM84SPHeLXP58EbE/CdyW5INJbgI+A5xcwBxvk2TPcOKEJHuAT7B8S1GfBI4Ot48Cjy5wlt+yLMt4b7XMOAt+7ha+/HlVzf0C3M36GfmfAn+9iBm2mOsPgf8cLs8tejbgYdbf1l1j/dzGvcB7gdPAOeDfgX1LNNs/Ac8Cz7Ae1oEFzXYH62/RnwHODJe7F/3cXWeuuTxv/lxWasITdFITxi41YexSE8YuNWHsUhPGLjVh7FIT/wd9DjwtMzSmggAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_image = sample.cuda() #+ torch.normal(.5, 0.16, size=input_tensor.shape).cuda()\n",
    "#input_image = torch.ones(size=sample.shape).cuda() * 255\n",
    "plt.imshow(input_image[0].permute(1, 2, 0).cpu().numpy())\n",
    "#all_images = scale_batch_images(all_images)\n",
    "label = torch.tensor([class_num]).cuda()\n",
    "#input_images = man_transform(all_images).cuda()\n",
    "probdict = prediction_probs(unsup.cuda(), input_image)\n",
    "print(\"The output cluster and the probability of them: \", probdict)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise base. It is the baseline for removing feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of noise image:  100\n"
     ]
    }
   ],
   "source": [
    "noise_images = torch.from_numpy(np.load('./noise_images.npy'))\n",
    "print(\"Number of noise image: \" , len(noise_images))\n",
    "# Use grey:\n",
    "noise_images = None\n",
    "#print(noise_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 1\n",
    "scale = 4\n",
    "lr = 0.5\n",
    "steps = 5000\n",
    "obj = 'xent'\n",
    "noise_bs = 10\n",
    "reg_l1 = 2e-05\n",
    "reg_tv = 0.01\n",
    "reg_ent = 0.0\n",
    "debug = True\n",
    "model = unsup.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28])\n",
      "Total classes in the model:  10\n",
      "1 4 0.5 5000 xent 10 2e-05 0.01 0.0\n",
      "4999: loss: 2.17, l1 norm: 5, tv: 0.03, ent: 0.03, pred prob: 0.2319\n"
     ]
    }
   ],
   "source": [
    "print(input_image.shape)\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_image.cuda())\n",
    "    output_probs = torch.nn.Softmax(dim = 1)(outputs).detach().cpu().numpy()\n",
    "print(\"Total classes in the model: \", len(output_probs[0]))\n",
    "#Force explnation\n",
    "output_labels = np.argmax(output_probs, axis = 1)\n",
    "output_labels = [1]\n",
    "probs = torch.zeros(input_image.shape[0], 200)\n",
    "for target_label in output_labels:\n",
    "    probs[:, target_label] = 1\n",
    "\n",
    "batch_masked_model = learn_masks_for_batch_Kcert(\n",
    "                model, input_image, target_probs=probs, K=K, scale=scale,\n",
    "                opt=optim.Adam, lr=lr, steps=steps, obj=obj,\n",
    "                noise_mean=None, noise_batch=noise_images, noise_bs=noise_bs,\n",
    "                reg_l1=reg_l1, reg_tv=reg_tv, reg_ent=reg_ent, old_mask=None, debug=debug)\n",
    "masks = batch_masked_model.mask().detach().cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0853, 0.2319, 0.0853, 0.0853, 0.0853, 0.0853, 0.0853, 0.0853, 0.0853,\n",
      "         0.0853]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "[{1: 0.23191052675247192}]\n"
     ]
    }
   ],
   "source": [
    "prob_dicts = []\n",
    "probs = batch_masked_model(model ,input_image, masks=masks.cuda()[0])\n",
    "print(probs)\n",
    "prediction_labels = torch.argmax(probs, axis = 1)\n",
    "for it in prediction_labels:\n",
    "    prob_dicts.append({it.item(): probs[0][it].detach().item()})\n",
    "print(prob_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 1, 28, 28])\n",
      "torch.Size([1, 1, 28, 28])\n",
      "tensor(1.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f30b44c52e8>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(masks.shape)\n",
    "heatmap = masks[0]\n",
    "neg_heatmap = masks[1]\n",
    "print(heatmap.shape)\n",
    "\n",
    "# normalize heatmap \n",
    "#heatmap = scale_batch_images(heatmap)\n",
    "\n",
    "#plt.close()\n",
    "heatmap = scale_batch_images(heatmap)\n",
    "print(torch.max(heatmap[0][0]))\n",
    "plt.imshow(heatmap.cpu().numpy()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AxesSubplot' object has no attribute 'flat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-158-d4fe8fc7b411>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# Display the grid of images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mplot_image_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0mplot_image_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-158-d4fe8fc7b411>\u001b[0m in \u001b[0;36mplot_image_grid\u001b[0;34m(images_array)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# Iterate through the axes and plot the images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m# Remove x-axis and y-axis ticks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AxesSubplot' object has no attribute 'flat'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def scale_image_to_255(image_array):\n",
    "    \"\"\"\n",
    "    Scales the input image array to the range [0, 255].\n",
    "\n",
    "    Args:\n",
    "        image_array (np.array): A NumPy array representing an image\n",
    "\n",
    "    Returns:\n",
    "        np.array: A new NumPy array with pixel values scaled to the range [0, 255]\n",
    "    \"\"\"\n",
    "    print(image_array)\n",
    "    # Find the minimum and maximum pixel values\n",
    "    min_val = np.min(image_array)\n",
    "    max_val = np.max(image_array)\n",
    "    # Scale the image array to the range [0, 255]\n",
    "    scaled_image_array = 255 * (image_array - min_val) / (max_val - min_val)\n",
    "\n",
    "    return scaled_image_array\n",
    "\n",
    "\n",
    "def plot_image_grid(images_array):\n",
    "    \"\"\"\n",
    "    Plots a grid of images from a NumPy array.\n",
    "\n",
    "    Args:\n",
    "    images_array (numpy.ndarray): A NumPy array of images with shape (num_images, height, width) or (num_images, height, width, channels)\n",
    "    \"\"\"\n",
    "\n",
    "    # Determine the grid size based on the number of images\n",
    "    grid_size = int(np.ceil(np.sqrt(len(images_array))))\n",
    "\n",
    "    # Create the figure and axes objects\n",
    "    fig, axes = plt.subplots(grid_size, grid_size, figsize=(10, 10))\n",
    "\n",
    "    # Iterate through the axes and plot the images\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < len(images_array):\n",
    "            # Remove x-axis and y-axis ticks\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "\n",
    "            # Plot the image on the current axis\n",
    "            if images_array.shape[1] == 1:\n",
    "                img = images_array[i][0] \n",
    "            else:\n",
    "                img = np.transpose(images_array[i], (1, 2, 0))\n",
    "\n",
    "            ax.imshow(img, cmap='gray' if images_array.shape[1] == 1 else None)\n",
    "        else:\n",
    "            # Remove the empty subplot\n",
    "            fig.delaxes(ax)\n",
    "\n",
    "    # Add space between the subplots\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "\n",
    "    # Display the grid of images\n",
    "    plt.show()\n",
    "plot_image_grid(heatmap)\n",
    "plot_image_grid(images.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def apply_masks_to_images(images_list, masks_list):\n",
    "    \"\"\"\n",
    "    Applies individual single-channel masks to a list of channel-first 3-channel images.\n",
    "\n",
    "    Args:\n",
    "        images_list (list of np.array): A list of channel-first 3-channel images, each with shape (3, height, width)\n",
    "        masks_list (list of np.array): A list of single-channel masks, each with shape (height, width)\n",
    "\n",
    "    Returns:\n",
    "        list of np.array: A list of masked channel-first images, each with shape (3, height, width)\n",
    "    \"\"\"\n",
    "\n",
    "    if len(images_list) != len(masks_list):\n",
    "        raise ValueError(\"The number of images and masks must be equal\")\n",
    "\n",
    "    masked_images = []\n",
    "\n",
    "    for image, mask in zip(images_list, masks_list):\n",
    "        if len(image.shape) != 3 or image.shape[0] != 1:\n",
    "            raise ValueError(\"Each input image must have a shape of (3, height, width)\")\n",
    "        print(mask.shape)\n",
    "        if len(mask.shape) != 3:\n",
    "            raise ValueError(\"Each input mask must have a shape of (channel, height, width)\")\n",
    "\n",
    "        # Expand the mask to match the shape of the image\n",
    "        s_mask = mask[0]\n",
    "        expanded_mask =  s_mask#.unsqueeze(0).repeat(3, 1, 1)\n",
    "        # Multiply the image with the expanded mask\n",
    "        masked_image = image * expanded_mask\n",
    "\n",
    "        # Add the masked image to the list\n",
    "        masked_images.append(masked_image)\n",
    "    masked_images = torch.stack(masked_images, axis=0)\n",
    "    return masked_images\n",
    "#plt.imshow(neg_heatmap[0][0])\n",
    "print(heatmap.shape)\n",
    "masked_image = apply_masks_to_images(input_image, heatmap.cuda())\n",
    "\n",
    "masked_image_vis = scale_batch_images(masked_image)\n",
    "\n",
    "\n",
    "plt.imshow(masked_image[0].permute(1, 2, 0).cpu().numpy())\n",
    "image = transforms.ToPILImage()(masked_image_vis[0])\n",
    "\n",
    "# Save the PIL image to a file\n",
    "image.save(\"image.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked images shape:  torch.Size([1, 1, 28, 28])\n",
      "Masked image max, min:  tensor(255., device='cuda:0') tensor(0., device='cuda:0')\n",
      "(1, 10)\n",
      "The output and the probability of them:  [{1: 0.23196931}]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Masked images shape: \",masked_image.shape)\n",
    "print(\"Masked image max, min: \", torch.max(masked_image), torch.min(masked_image))\n",
    "masked_input = masked_image.clone().detach().cuda()\n",
    "#acc = evaluate(model,masked_image, label)\n",
    "probdict = prediction_probs(model, masked_image)\n",
    "print(\"The output and the probability of them: \", probdict)\n",
    "plt.imshow(masked_image[0].permute(1, 2, 0).cpu().numpy())\n",
    "plt.axis(\"off\")\n",
    "#plt.title(f\"{data_dict[str(list(probdict[0].keys())[0])][1]}:{list(probdict[0].values())[0]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-36dfea7bf74e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmasked_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmasked_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmasked_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprobdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmasked_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The output and the probability of them: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-58-e5bbac1a0446>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, images, labels)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0moutput_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0moutput_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_labels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\" Accuracy: {correct}/{total}: {correct / total}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "masked_input = masked_image.clone().detach().cuda()\n",
    "acc = evaluate(model,masked_image, label)\n",
    "probdict = prediction_probs(model,masked_image)\n",
    "print(\"The output and the probability of them: \", probdict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masked images shape:  torch.Size([1, 3, 224, 224])\n",
    "Masked image max, min:  tensor(0.1331) tensor(0.)\n",
    " Accuracy: 1/1: 1.0\n",
    "The output and the probability of them:  [{782: 0.99647576}]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
